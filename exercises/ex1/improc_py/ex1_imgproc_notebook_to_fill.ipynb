{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction / Review of basic Digital Image Processing in Python (using OpenCV and numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **goal** of this practice session is to get familiarized with python and standard scientific computing packages used for computer vision (such as [OpenCV](https://opencv.org/) and [numpy](https://numpy.org/)) in the context of performing basic image processing operations, such as filtering.\n",
    "\n",
    "If you already know these tools, you can skip this session. Its intention is that all of us in class are \"on the same page\" for the next exercises.\n",
    "\n",
    "# Python\n",
    "\n",
    "Since we will use ROS ([Robot Operating System](https://www.ros.org/) - Kinetic or Melodic versions) and it is starting to transition toward using Python 3 in its [Noetic](http://wiki.ros.org/UsingPython3) version, we review here using Python 2.7. Feel free to adapt the code to Python 3 if you want (and let your colleagues in class know about it).\n",
    "\n",
    "Hence, we use Python 2.7 and numpy, opencv2 packages in Linux (e.g., Ubuntu LTS).\n",
    "\n",
    "There are several ways to install these. \n",
    "\n",
    "- Installation of numpy using pip: `sudo apt update; sudo apt install python-pip; pip install numpy`\n",
    "- Installation of opencv using pip: `pip install opencv-python`\n",
    "\n",
    "See [opencv-python](ttps://pypi.org/project/opencv-python/)\n",
    "\n",
    "You may also install OpenCV using a package manager, such as `Synaptic Package Manager`.\n",
    "Â \n",
    "\n",
    "**ROS**: Another possible way to install OpenCV that will be useful for future sessions is to get it from ROS. The [Desktop Install version of ROS](http://wiki.ros.org/kinetic/Installation/Ubuntu) should suffice. Both versions, ROS Kinetic (2016) or ROS Melodic (2018), should work. Example installation command: `sudo apt-get install ros-kinetic-desktop`\n",
    "\n",
    "In your own time, feel free to go beyond the exercises in this session. You may follow a basic tutorial in python and numpy to know how to create variables and how to access their content. You may follow OpenCV tutorials to play around with image data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script\n",
    "\n",
    "For this exercise we use the Python code in the script `ex1_improc.py` \n",
    "\n",
    "The script is incomplete. The parts marked with `???` need to be completed for proper execution.\n",
    "\n",
    "With the script we aim to learn the following:\n",
    "- How to load an image an image from disk\n",
    "- How to write an image to disk (image which may include negative values)\n",
    "- How to plot image without visual artefacts\n",
    "- What the spatial and intensity resolutions of an image are\n",
    "- How to linearly filter an image with a kernel\n",
    "- How to compute derivatives of image data\n",
    "  - In space, within an image (Sobel operators)\n",
    "  - In time, using two images (forward difference formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Digital Image Processing\n",
    "\n",
    "A classic reference on Digital Image Processing is the book by Gonzalez and Woods, [\"Digital Image\n",
    "Processing\"](http://www.imageprocessingplace.com/DIP-4E/dip4e_main_page.htm), now in its 4th edition (2018), which contains MATLAB code. The same authors also have a more practical book called \"Digital Image Processing Using MATLAB\" (3rd edition). These are just some references in case you want to read more about image processing.\n",
    "\n",
    "## Digital Images\n",
    "Monochrome (grayscale) images can be modeled by two-dimensional functions $f:\\mathbb{R}^{2}\\to\\mathbb{R}$. The amplitude of $f$ at spatial coordinates $(x,y)$, i.e., $f(x,y)$, is called the *intensity* or \\emph{gray level} at that point, and it is related to a physical quantity by the nature of the image acquisition device; for example, it may represent the energy radiated by a physical source. We will deal with bounded (i.e., finite) quantities, and so $|f|<\\infty$. Common practice is to perform an affine transformation (substituting $f\\leftarrow af+b$ for all $(x,y)$ by means of some suitable constants $a,b$) so that $f$ takes values in a specified interval, e.g., $f\\in[0,1]$.\n",
    "\n",
    "**A digital image** can be modeled as obtained from a continuous image $f$ by a conversion process having two steps: sampling (digitizing the spatial coordinates $x,y$) and quantization (digitizing the amplitude $f$). Therefore, a digital image may be represented by an array of numbers, $M=(m_{ij}),$ where $i,j$ and $m$ can only take a finite number of values, e.g., $i=\\{0,1,\\ldots,W-1\\}$, $j=\\{0,1,\\ldots,H-1\\}$ and $m=\\{0,1,\\ldots,L-1\\}$ for some positive integers $W,H,L$ (Width, Height and number of gray Levels). That is, a digital image is a 2-D function whose coordinates and amplitude values are discrete (e.g., integers). Specifically,  $$m_{ij}=q\\bigl(f(x,y)\\bigr)=q\\bigl(f(x_{0}+i\\cdot\\Delta x,\\,y_{0}+j\\cdot\\Delta y)\\bigr),$$\n",
    "where $\\Delta x$ and $\\Delta y$ are the sampling steps in a grid with spatial coordinates starting at some location $(x_{0},y_{0})$, and $q:\\mathbb{R}\\to\\{0,\\ldots,L-1\\}$ is the input-output function of the quantizer (which has a staircase shape).\n",
    "\n",
    "Common practice for grayscale images is to use 1 byte to represent the intensity at each location $(i,j)$ (i.e., picture element or \"pixel\"). Since 1 byte = 8 bits, the number of possible gray levels is $L=2^{8}=256$, and so intensities range from $i=0$ (black) to $i=L-1=255$ (white). Hence, images are typical stored and read as unsigned character (unsined int8 variables). However, to numerically operate with grayscale images, it is convenient to convert the data type of the image values from integers to real numbers, i.e., from 8 bits to single or double precision (float type). Once operations are finished, it may be convenient to convert back to 8 bit format for storage of the resulting image, thus producing a quantization of the data values. Medical images are usually represented with a larger depth (10-12 bits) to mitigate the occurrence of visual artifacts due to the quantization process.\n",
    "\n",
    "Color images can be represented, according to the human visual system, by the combination of three monochrome images (with the amount of red (R), green (G) and blue (B) present in the image), and so, each pixel is represented by 3 bytes, which provides a means to describe $2^{3\\times8}\\approx16.8$ million different colors. Many of the techniques developed for monochrome images can be extended to color images by processing the three component images individually.\n",
    "\n",
    "The number of bits required to store a (grayscale) digital image is $b=W \\cdot H \\cdot \\log_{2}(L)$, and so compression algorithms (such as JPEG) are essential to reduce the storage requirement by exploiting and removing redundancy of the image.\n",
    "\n",
    "**Spatial resolution** is a measure of the smallest discernible detail in an image, and it is usually given in dots (pixels) per unit distance. That is, it is the density of pixels over the image, but informally, the image size ($W\\times H$ pixels) is regarded as a measure of spatial resolution (although it should be stated with respect to physical units - cm, etc.). It is common to refer to the number of bits used to quantize intensity as the **intensity resolution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show how to load an image, such as ![Tiger](images/tigerg.jpg)\n",
    "Standard images can be downloaded from [standard_test_images.zip](http://www.imageprocessingplace.com/downloads_V3/root_downloads/image_databases/standard_test_images.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to save an image with positive and negative values to disk\n",
    "mapping the zero values to the middle gray level (about level 128 in [0,255])\n",
    "  - maxabsval: maximum absolute value of input image src\n",
    "\"\"\"\n",
    "def imwriteSymmetricImage(filename, src, maxabsval=None):\n",
    "    \n",
    "    if not maxabsval:\n",
    "        maxabsval = np.amax(np.abs(src))\n",
    "       \n",
    "    # Conversion form the interval [-1,1]*maxabsval to [0,255]\n",
    "    #   the value -maxabsval maps to 0\n",
    "    #   the value  maxabsval maps to 255\n",
    "    img_normalized = ???\n",
    "    cv2.imwrite(filename, img_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to plot without artefacts.\n",
    "By default, imshow on my notebook produces image aliasing (https://en.wikipedia.org/wiki/Aliasing)\n",
    "So this function tries to mitigate that effect in the jupyter notebook, displaying at the actual iamge size\n",
    "\"\"\"\n",
    "def myimshow(src):\n",
    "    dpi = 80\n",
    "    h, w = src.shape\n",
    "    fig = plt.figure(figsize=(h/float(dpi), w/float(dpi)), dpi=dpi)\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.imshow(src, extent=(0,w,h,0), interpolation=None, cmap='gray')\n",
    "    ax.set_xticks([])  # remove xticks\n",
    "    ax.set_yticks([])  # remove yticks\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load image from disk\n",
    "To read an image from disk, we use the OpenCV command `imread`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/barbara.png', cv2.IMREAD_GRAYSCALE)\n",
    "# img = cv2.imread('images/tigerg.jpg', cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will return a variable called `img` of type uint8 (unsigned integer represented by 8 bits = 1 byte) and it is a multi-dimensional array/matrix of size $N_{\\text{rows}}\\times N_{\\text{cols}}\\times N_{\\text{channels}}$. \n",
    "If $N_{\\text{channels}}=3$ there are three channels or \"bands\", thus, it is a color image (typically in RGB format). In the additional arguments of the function we indicate to read as grayscale image, so it only has one channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may check the type of the variables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))  # numpy ndarray\n",
    "print(img.dtype)  # unit8 or float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the size of the image by using the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways of printing values on screen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print img.shape\n",
    "print height, width\n",
    "print(\"height=\",height,\" width=\",width)\n",
    "print(\"height= {}, width={}\".format(height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert to floating point values (for numerical manipulation), so that we are not restricted to operations with only 8 bits (256 representation levels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.astype(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may check again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(img))  # numpy ndarray\n",
    "print(img.dtype)  # unit8 or float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying an image. Coordinate axes convention\n",
    "\n",
    "To visualize the image that has been previously loaded in variable img, you may use [matplotlib.pyplot's](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.html) commands imshow or figimage. \n",
    "The default options of the command imshow causes some artefacts (e.g., [moire patterns](https://en.wikipedia.org/wiki/Moir%C3%A9_pattern) by resizing the image, so we use the command figimage instead.\n",
    "In this jupiter notebook, we defined the function `myimshow` to try to avoid the visualization artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myimshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a new window and display the image. By default, `imshow` or `figimage` do not use a grayscale colormap, so the image will be colored (even though it is supposed to be a grayscale image). Investigate the [documentation of figimage or imshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.figimage.html) to change the colormap from the default one to a grayscale one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In images, the coordinate system origin is at the **upper left corner**, with the positive y axis extending downward (along the rows of the matrix variable img) and the positive x axis extending to the right (along the columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(???, cmap='gray', interpolation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Region of Interest (ROI) within an image\n",
    "\n",
    "To select a subregion of an image (also called ROI - Region Of Interest) we may use Python [slice notation](https://stackoverflow.com/questions/509211/understanding-slice-notation). \n",
    "For example, to select the top-left $100\\times200$-pixel region of `img` we type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sub = img[ 0:100, 0:200 ]\n",
    "myimshow(img_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example to select a central region of the image by cropping 20 pixels from the top and the bottom and 50 pixels from each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using variables to specify the ROI (border cropping, in this case)\n",
    "border_r = 20 # row dimension\n",
    "border_c = 50 # column dimension\n",
    "img_sub2 = img[ border_r:-border_r, border_c:-border_c ]\n",
    "myimshow(img_sub2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing an image to disk\n",
    "To save an image to disk, we use the OpenCV command `imwrite`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('SavedImage.png', img_sub2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a file called `SavedImage.png` in the working folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial resolution\n",
    "\n",
    "Given a well-sampled digital image over a pixel grid, the effect of reducing the spatial resolution, i.e., the number of pixels (while keeping the number of intensity levels constant) is shown next.\n",
    "\n",
    "The image quality degrades as it is represented with fewer number of pixels. The coarsening effect or \"pixelization\" is evident when an insufficient number of pixels is used; such number of samples cannot capture the fine details present in the scene.\n",
    "\n",
    "You would need to know how to use the OpenCV command `cv2.resize` and the pyplot function `imshow` to complete the loop that generates the figure. The loop variable called scale\\_factor indicates the value by which the original image is downsized (in each dimension, $x,y)$) to get each image in the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))\n",
    "for ax, k in zip(axs.flat, range(6)):\n",
    "    scale_factor = 1./(2**k)\n",
    "    img_small = cv2.resize(img, ???)\n",
    "    ax.imshow(img_small, interpolation=???, cmap=???)\n",
    "    ax.set_title(\"Scale: 1 / \" + str(2**k))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grayscale (range) resolution. \n",
    "Number of intensity levels, number of bits (bit depth). Artefacts\n",
    "\n",
    "Next, we take a look at the effect of maintaining the spatial resolution but decreasing the intensity resolution. That is, we maintain the number of image pixels, but we use fewer bits to encode them.\n",
    "\n",
    "For this part of the Python code, you would need to remember that division of integer-type variables produces integers and therefore discards fractional information, effectively producing a quantization effect. Feel free to use the following code or design simpler ways to generate a figure like the following one by processing the original image (in the top left of the figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(20, 10))\n",
    "for ax, k in zip(axs.flat, range(6)):\n",
    "    size_quant_interval = 2**k\n",
    "    img_quantized = (img.astype(int) / size_quant_interval) * size_quant_interval\n",
    "    ax.imshow(img_quantized, ???)\n",
    "    ax.set_title(\"Num gray levels: \" + str(256/size_quant_interval))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure... \n",
    "- Can you discern the differences between the resulting images?\n",
    "- How many intensity levels does the human visual system need in order to represent the a natural scene faithfully? (You would need to test with multiple images).\n",
    "\n",
    "Observe that, as the number of levels decreases, *false contours* appear in smooth intensity regions. In the bottom right image of the figure above, can you discern the 8 intensity levels used to represent the image?\n",
    "\n",
    "It may be good to use the standard image called \"Lena\" because it has both regions of smooth intensity variation (i.e., \"low spatial frequencies\"), such as the shoulder, and regions of large intensity variations (\"high spatial frequencies\"), such as strong edges and the plumage accompanying the hat. It is good for testing purposes. Feel free to run the code on other images that have these two ingredients.\n",
    "\n",
    "Which test image would you use to clearly see the number of reduced intensity levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscale ramp image\n",
    "x = np.arange(0, 255, 0.4)\n",
    "y = np.arange(0, 255, 0.666)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "img_ramp = xx\n",
    "myimshow(img_ramp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(9, 6))\n",
    "for ax, k in zip(axs.flat, range(6)):\n",
    "    size_quant_interval = 2**k\n",
    "    img_quantized = (img_ramp.astype(int) / size_quant_interval) * size_quant_interval\n",
    "    ax.imshow(img_quantized, interpolation='none', cmap='gray')\n",
    "    ax.set_title(\"Num gray levels: \" + str(256/size_quant_interval))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering (convolution)\n",
    "\n",
    "### 1-D (Linear) Filtering. Convolution\n",
    "\n",
    "The convolution of two sequences of numbers (\"signals\") $a[n]$ and $b[n]$, $n\\in\\mathbb{Z}$, is symbolized by $c=a\\star b$ and calculated according to\n",
    "$$c[n]=\\sum_{k=-\\infty}^{\\infty}a[k]b[n-k].\\label{eq:convolutionDef}$$\n",
    "The convolution is commutative ($a\\star b=b\\star a$), so $a$ and $b$ can be swapped in the previous formula. In practice, we use sequences of finite length, so the summation in previous formula  is carried over a finite number of products.\n",
    "\n",
    "A demonstration of the convolution is illustrated in the movie [convolution.mp4](videos/convolution.mp4), where two signals $a,b$ are defined and their convolution is computed. \n",
    "In the video, one signal (middle plot, in blue) is multiplied by a reversed and shifted version (middle plot, red) of the other signal, and the sum of the product gives one sample of the output signal (bottom, in black).\n",
    "\n",
    "Linear filtering is implemented by linear shift-invariant systems, whose output consists of the convolution of the input signal ($a$) and the impulse response of the filter ($b$), i.e., $c=a\\star b$. For images, we will be using the 2-D convolution implemented in OpenCV's command [cv2.filter2D](https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#filter2d).\n",
    "\n",
    "\n",
    "### 2-D (Linear) Filtering.\n",
    "\n",
    "The convolution operation can be extend to two-dimensional discrete signals, i.e., monochrome images. The convolution of two digital images $h,f$ is written as $g=h\\star f$ and calculated by \n",
    "$$g[i,j]=\\sum_{u}\\sum_{v}h[u,v]f[i-u,j-v].\\label{eq:ConvTwoDimDef}$$\n",
    "\n",
    "Often, $h$ is the filter (called \"kernel\" or \"mask\") and $f$ is the input image. Then, due to the commutativity of the convolution, $g=h\\star f=f\\star h$, it is standard to think of $g$ as computed by reversing and shifting $h$ (the kernel) rather than by reversing and shifting the input signal $f$ in the previous formula.\n",
    "\n",
    "Each output pixel is computed by a weighted sum of its neighbors in\n",
    "the input image, and if $h$ is a filter with a kernel of size $n\\times n$\n",
    "pixels, this is the size of the neighborhood, and it implies $n^{2}$\n",
    "multiplications. For example, if $h$ is a filter with Gaussian shape\n",
    "(samples of a Gaussian function on an $n\\times n$ grid), each output\n",
    "pixel is computed as a Gaussian weighted average of the input pixels\n",
    "in the neighborhood. The output $g$ will be smoother than the input\n",
    "$f$ since high frequencies have been attenuated by the filter $h$.\n",
    "You could see this by plotting the Fourier Transform of $h$, but\n",
    "this is optional for you to show.\n",
    "\n",
    "*Separable filters* are of special importance since they save a lot\n",
    "of computations. A filter is separable if its kernel can be written\n",
    "as the product of two independent 1-D kernels, i.e., $h[u,v]=p[u]q[v]$.\n",
    "That is, the matrix $h[u,v]$ can be written as the (exterior) product\n",
    "of two vectors $p[u]$ and $q[v]$. This reduces the cost of convolution;\n",
    "the filtered image can be obtained as a cascade of the two 1-D convolutions,\n",
    "with $2n$ multiplications per pixel instead of the $n^{2}$ count\n",
    "in the general, non-separable case. This is internally handled by\n",
    "OpenCV in popular filters, and there is a specific function for that\n",
    "in case you are curious about it: [cv2.sepFilter2D](https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#sepfilter2d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box filter\n",
    "Let us use a 5x5 kernel size that will sum all the values in the image and then divide by 25, that is, that will average all the intensity values in the image as the kernel sweeps through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel of 5x5 size\n",
    "# kernel = np.ones((5,5),np.float32)/25\n",
    "# img_box = cv2.filter2D(img,-1,kernel)\n",
    "img_box = cv2.blur(img, (5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now display the original and the filtered images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myimshow(img)\n",
    "myimshow(img_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing by Gaussian filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 2. # Parameter that controls the amount of smoothing\n",
    "if s > 0:\n",
    "    img_gauss = cv2.GaussianBlur(img, ???)\n",
    "\n",
    "myimshow(img_gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise filtering\n",
    "### Additive Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Noise (of the same size as the image)\n",
    "img_noise = 0*img\n",
    "cv2.randn(img_noise,(0),(15))\n",
    "myimshow(img_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the clean image to obtain a noisy image that we want to filter\n",
    "img_corrupted = img + img_noise\n",
    "# (we use floats instead of 8-bit numbers to represent intensity levels)\n",
    "myimshow(img_corrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter image using a linear filter\n",
    "s = 1.5 # Parameter that controls the amount of smoothing\n",
    "img_filtered = cv2.GaussianBlur(img_corrupted, (0,0), sigmaX = s, sigmaY = s)\n",
    "myimshow(img_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salt-and-pepper noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add salt-and-pepper noise\n",
    "img_noise = 0*img\n",
    "cv2.randu(img_noise,(0),(1))\n",
    "img_corrupted = img.copy() # create a copy\n",
    "img_corrupted[img_noise < .03] = 0.\n",
    "img_corrupted[img_noise > 0.97] = 255.\n",
    "myimshow(img_corrupted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler the image using a linear filter\n",
    "img_filtered = cv2.GaussianBlur(img_corrupted, (0,0), sigmaX = s, sigmaY = s)\n",
    "myimshow(img_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler the image using a median filter (it is a non-linear filter)\n",
    "# For the median filter in OpenCV the input cannot be float\n",
    "cv2.convertScaleAbs(img_corrupted, img_corrupted_8bits)\n",
    "img_median = cv2.medianBlur(img_corrupted_8bits,3)\n",
    "myimshow(img_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which of the two above filters (Gaussian, median) works better against salt-and-pepper noise?\n",
    "- Which one works better against zero-mean additive Gaussian noise?\n",
    "- Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial derivatives. Sobel operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_x = cv2.Sobel(img, cv2.CV_64F, 1,0)\n",
    "grad_y = cv2.Sobel(img, cv2.CV_64F, 0,1)\n",
    "\n",
    "myimshow(grad_x)  # Image gradient x\n",
    "myimshow(grad_y)  # Image gradient y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Plot the filter mask or signal or kernel used\n",
    "ksize = 5  # Size of the impulse image\n",
    "impulse_img = np.zeros((ksize,ksize))\n",
    "impulse_img[ksize/2,ksize/2] = 1\n",
    "sobel_response_to_impulse = cv2.Sobel(impulse_img, cv2.CV_64F, 1,0)\n",
    "print sobel_response_to_impulse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to scale the gradient appropriately so that they have meaning of derivative\n",
    "grad_x /= 8\n",
    "grad_y /= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Write derivative images to disk\n",
    "\n",
    "# Plot histogram\n",
    "fig = plt.figure()\n",
    "plt.hist(grad_x.ravel(), bins=101)\n",
    "plt.title('Histogram of grad_x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing to disk (using unsigned 8 bits), we need to convert the range of the derivative images, from $[-b,b]$ to $[0,255]$. To compute the min and max of the image, check out the documenation of the OpenCV function [cv2.minMaxLoc](https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#minmaxloc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from (-b,b) to (0,255)\n",
    "#min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(grad_x)\n",
    "??? = cv2.minMaxLoc(grad_x)\n",
    "??? = cv2.minMaxLoc(grad_y)\n",
    "M_val = np.amax([np.abs(min_valx), max_valx, np.abs(min_valy), max_valy])\n",
    "print M_val\n",
    "imwriteSymmetricImage('grad_x.png', grad_x, M_val)\n",
    "imwriteSymmetricImage('grad_y.png', grad_y, M_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Convert gradient (x,y) to magnitude and phase\n",
    "grad_mag, grad_dir = cv2.cartToPolar(grad_x, grad_y)\n",
    "\n",
    "# Alternative commands\n",
    "#grad_mag = np.hypot(grad_x, grad_y) \n",
    "#grad_dir = np.arctan2(grad_y,grad_x)\n",
    "\n",
    "myimshow(grad_mag)  # Gradient magnitude\n",
    "myimshow(grad_dir)  # Gradient direction\n",
    "\n",
    "cv2.imwrite('grad_mag.png', grad_mag * 255 / np.amax(grad_mag))\n",
    "imwriteSymmetricImage('grad_dir.png', grad_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD slider_depth sequence (txt format) from DAVIS dataset\n",
    "import wget\n",
    "print('Beginning file download with wget...')\n",
    "url = 'http://rpg.ifi.uzh.ch/datasets/davis/slider_depth.zip'\n",
    "wget.download(url, 'slider_depth.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the dataset, **unzip it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "id0 = 59\n",
    "filename_prefix = 'slider_depth/images/frame_'\n",
    "filename_suffix = '.png'\n",
    "filename1 = filename_prefix + (\"%08d\" % id0) + filename_suffix\n",
    "filename2 = filename_prefix + (\"%08d\" % (id0+1)) + filename_suffix\n",
    "\n",
    "# Which means:\n",
    "#img1 = cv2.imread('/home/ggb/improc_py/slider_depth/images/frame_00000059.png', cv2.IMREAD_GRAYSCALE)\n",
    "#img2 = cv2.imread('/home/ggb/improc_py/slider_depth/images/frame_00000060.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Read two consecutive images\n",
    "img1 = cv2.imread(filename1, cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(filename2, cv2.IMREAD_GRAYSCALE)\n",
    "img1 = img1.astype(float)\n",
    "img2 = img2.astype(float)\n",
    "height, width = img1.shape\n",
    "print height, width\n",
    "\n",
    "# Compute the (approximation to the) temporal derivative\n",
    "img_difference = ???\n",
    "\n",
    "# Display two images and the temporal derivative\n",
    "myimshow(img1)\n",
    "myimshow(img2)\n",
    "myimshow(img_difference)\n",
    "\n",
    "# Saving images to disk using the same symmetric rage for all three derivatives\n",
    "grad_x = cv2.Sobel(img2, cv2.CV_64F, 1,0)\n",
    "grad_y = cv2.Sobel(img2, cv2.CV_64F, 0,1)\n",
    "min_valx, max_valx, _, _ = cv2.minMaxLoc(grad_x)\n",
    "min_valy, max_valy, _, _ = cv2.minMaxLoc(grad_y)\n",
    "min_valt, max_valt, _, _ = cv2.minMaxLoc(img_difference)\n",
    "M_val = np.amax([np.abs(min_valx), max_valx, np.abs(min_valy), max_valy, np.abs(min_valt), max_valt])\n",
    "print M_val\n",
    "imwriteSymmetricImage('grad_x.png', grad_x, M_val)\n",
    "imwriteSymmetricImage('grad_y.png', grad_y, M_val)\n",
    "imwriteSymmetricImage('grad_t.png', img_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other topics to practice\n",
    "\n",
    "The median and bilateral filters are popular non-linear filters in image processing. These do not follow a convolution formula.\n",
    "\n",
    "**References**:\n",
    "- [py_basic_ops](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_core/py_basic_ops/py_basic_ops.html)\n",
    "- [py_filtering](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_filtering/py_filtering.html#filtering)\n",
    "- [OpenCV Smoothing Images](https://docs.opencv.org/2.4/doc/tutorials/imgproc/gausian_median_blur_bilateral_filter/gausian_median_blur_bilateral_filter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img8 = cv2.imread('images/tigerg.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# For the median filter in OpenCV the input cannot be float\n",
    "img_median = cv2.medianBlur(img8,7)\n",
    "myimshow(img_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_blur = cv2.bilateralFilter(img8,7,75,75)\n",
    "myimshow(img_blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
